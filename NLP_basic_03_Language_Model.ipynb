{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "written by ideajoon<br/>\n",
    "※ 참고 : 딥 러닝을 이용한 자연어 처리 입문 (https://wikidocs.net/book/2155) 자료를 공부하고 정리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 언어 모델(Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. 언어 모델(Language Model)이란?\n",
    "2. 통계적 언어 모델(Statistical Language Model, SLM)\n",
    "3. N-gram Language Model\n",
    "4. 한국어에서의 언어 모델(Language Model for Korean Sentences)\n",
    "5. 펄플렉서티(Perplexity)\n",
    "6. 조건부 확률(Conditional Probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 언어 모델(Language Model)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델(Language Model)은 주어진 단어들로부터 다음에 등장할 단어의 확률을 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 단어의 시퀀스의 확률을 예측하는 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "음성 인식, 기계 번역, OCR, 검색어 자동 완성 등과 같은 것은 전부 언어 모델을 통해 이루어 집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델을 만드는 방법은 크게는 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최근 핫한 딥 러닝 자연어 처리의 신기술인 GPT나 BERT가 전부 언어 모델의 개념을 사용하여 만들졌음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 문장의 확률을 예측하는 일이 왜 필요한가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 기계 번역(Machine Translation):<br/>\n",
    "P(나는 버스를 탔다) > P(나는 버스를 태운다)<br/>\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 오타 교정(Spell Correction):<br/>\n",
    "선생님이 교실로 부리나케<br/>\n",
    "P(달려갔다) > P(잘려갔다)<br/>\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 음성 인식(Speech Recognition)\n",
    "P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)<br/>\n",
    ": 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델은 위와 같이 보다 적절한 문장을 선택하는 일에 확률을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 언어 모델은 문장의 확률 또는 단어 등장 확률을 예측하는 일."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. 문장의 확률 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 단어(word)를 w라고 합시다.<br/> 단어의 시퀀스인 전체 문장(sentence)을 대문자 W라고 합시다.<br/> n개의 w로 구성된 문장 W의 확률은 다음과 같이 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P(W) = P(w_1, w_2, w_3, w_4, w_5, ... ,w_n) = \\prod_{n=1}^{n}P(w_{n})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. 다음 단어 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-1개의 단어가 나열된 상태에서 n번째 단어의 확률을 다음과 같이 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P(w_n | w_1, ..., w_{n-1})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|의 기호는 조건부 확률(conditional probability)을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 언어 모델의 간단한 직관"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 검색 엔진에서의 언어 모델의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색엔진은 언어 모델을 이용하여 입력된 단어들의 나열에 대해서 다음 단어를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/21668/%EB%94%A5_%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 통계적 언어 모델(Statistical Language Model, SLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 조건부 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(B|A) = P(A,B)/P(A)$<br/>\n",
    "$P(A,B) = P(A)P(B|A)$<br/>\n",
    "$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ chin rule<br/>\n",
    "$P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 실제 문장을 통한 확률식 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An adorable little boy is spreading smiles이라는 문장 하나에 대한 전체 확률식을 구해보겠습니다. 다시 말해 P(An adorable little boy is spreading smiles)를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\prod_{n=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 문장에 해당 식을 적용해보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\text{An adorable little boy is spreading smiles}) =$<br/>\n",
    "$P(\\text{An})  ×  P(\\text{adorable|An})  ×  P(\\text{little|An adorable})  ×  P(\\text{boy|An adorable little}) ×  P(\\text{is|An adorable little boy}) ×  P(\\text{spreading|An adorable little boy is})  ×  P(\\text{smiles|An adorable little boy is spreading})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 카운트 기반의 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An adorable little boy가 나왔을 때, is가 나올 확률인 P(is|An adorable little boy) 계산한다고 해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이때 기계에게 P(is|An adorable little boy)를 묻는다면 기계는 30%라는 확률을 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 카운트 기반 접근의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스. 즉, 다시 말해 기계가 훈련하는 데이터는 방대한 양이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계가 훈련한 코퍼스에 An adorable little boy is라는 문장 자체가 없었다고 한다면 이 문장에 대한 확률을 계산할 수 없습니다. 단어 분리(subword segmentation) 챕터에서 배웠던 OOV 문제입니다. 코퍼스에 해당 데이터가 없다는 것은 카운트 계산 시 분자 또는 분모가 0이 된다는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 문제를 완전히 해결할 수는 없지만, 그래도 완화하는 방법들이 있습니다. 바로 n-gram 언어 모델과 여러가지 SLM의 일반화(generalization) 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 코퍼스에서 카운트하지 못하는 경우의 감소."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마르코프의 가정을 사용하면 SLM의 한계(훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다)를 극복하여 카운트를 할 수 있을 가능성을 높일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|boy})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|little boy})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 세지말고, 앞 단어 중 임의의 개수만 포함해서 세보자는 것입니다. 이렇게 하면 갖고있는 코퍼스에서도 해당 단어의 나열을 카운트할 확률이 높아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram은 n개의 연속적인 단어 나열을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unigrams : an, adorable, little, boy, is, spreading, smiles\n",
    "- bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
    "- trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "- 4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명하고 n이 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 합시다. 이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/21692/n-gram.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w\\text{|boy is spreading}) = \\frac{\\text{count(boy is spreading}\\ w)}{\\text{count(boy is spreading)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했다고 합시다. 그리고 boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 합시다. 그렇게 되면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%입니다. 확률적 선택에 따라 우리는 insults가 더 맞다고 판단하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\text{insults|boy is spreading}) = 0.500$<br/>\n",
    "$P(\\text{smiles|boy is spreading}) = 0.200$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) N-gram Language Model의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다는 점입니다. 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) n을 선택하는 것은 trade-off 문제."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n을 크게하면 언어 모델의 성능을 높일 수 있지만, 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, n-gram에 대한 OOV(Out Of Vocabulary) 문제가 발생할 수 있습니다. 또한 n이 커질수록 모델 사이즈는 굉장히 커지게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n을 너무 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 점점 실제의 확률분포와 멀어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 언급한 trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 카운트 했을 때 0이 되는 문제(zero count problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " n-gram 언어 모델도 여전히 n-gram에 대한 OOV 문제가 존재합니다. 결국 n-gram 언어 모델도, 확률이 0이 되는 또는 확률 자체를 계산할 수 없는 문제를 완전히 피할 수는 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 적용 분야(Domain)에 맞는 코퍼스의 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가령, 마케팅 분야에서는 마케팅 단어가 빈번하게 등장할 것이고, 의료 분야에서는 의료 관련 단어가 당연히 빈번하게 등장합니다. 이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " N-gram Language Model보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 한국어에서의 언어 모델(Language Model for Korean Sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 한국어는 어순이 중요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어순이 중요하지 않다는 것은 어떤 단어든 나타나도 된다는 의미입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어보도록 합시다.\n",
    "\n",
    "Ex)\n",
    "- ① 나는 운동을 합니다 체육관에서.\n",
    "- ② 나는 체육관에서 운동을 합니다.\n",
    "- ③ 체육관에서 운동을 합니다.\n",
    "- ④ 나는 운동을 체육관에서 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 단어 순서를 뒤죽박죽으로 바꾸어놔도 한국어는 의미가 전달 되기 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기가 어렵습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 한국어는 교착어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교착어인 한국어에는 조사가 있습니다. 영어는 기본적으로 조사가 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가령 '그녀'라는 단어 하나만 해도 그녀가, 그녀를, 그녀의, 그녀와, 그녀로, 그녀께서, 그녀처럼 등과 같이 다양한 경우가 존재합니다. 그렇기 때문에, 한국어에서는 토큰화를 통해 접사나 조사 등을 분리하는 것은 중요한 작업이 되기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 한국어는 띄어쓰기가 제대로 지켜지지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어는 띄어쓰기를 제대로 하지 않아도 의미가 전달되며, 띄어쓰기 규칙 또한 상대적으로 까다로운 언어이기 때문에 자연어 처리를 하는 것에 있어서 한국어 코퍼스는 띄어쓰기가 제대로 지켜지지 않는 경우가 많습니다. 이 경우에 토큰이 제대로 분리 되지 않아 언어 모델은 제대로 동작하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 펄플렉서티(Perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "펄플렉서티(Perplexity)는 언어 모델을 평가하기 위한 내부 평가 지표입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 언어 모델의 평가 방법(Evaluation metric) : PPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어에서 'perplexed'는 '헷갈리는'과 유사한 의미를 가집니다. 그러니까 여기서 PPL은 '헷갈리는 정도'로 이해해 봅시다. '낮을 수록' 언어 모델의 성능이 좋다는 것을 의미한다는 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL은 단어의 수로 정규화(normalization) 된 테스트 데이터에 대한 확률의 역수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL을 최소화한다는 것은 문장의 확률을 최대화하는 것과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 W의 길이가 N이라고 하였을 때의 PPL은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 확률에 체인룰(chain rule)을 적용하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PPL(W)=\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{1}, w_{2}, ... , w_{i-1})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에 n-gram을 적용해볼 수도 있습니다. 예를 들어 bigram 언어 모델의 경우에는 식이 아래와 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PPL(W)=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{i-1})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 분기 계수(Branching factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가령, 언어 모델에 어떤 테스트 데이터을 주고 측정했더니 PPL이 10이 나왔다고 해봅시다. 그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time-step)마다 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\frac{1}{N}}=(\\frac{1}{10}^{N})^{-\\frac{1}{N}}=\\frac{1}{10}^{-1}=10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델의 PPL은 테스트 데이터에 의존하므로 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많고, 또한 도메인에 알맞은 동일한 테스트 데이터를 사용해야 신뢰도가 높다는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 기존 언어 모델 Vs. 인공 신경망을 이용한 언어 모델."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL로 성능 테스트를 한 표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/21697/ppl.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 조건부 확률(Conditional Probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이해를 위한 간단한 예제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/21681/%EC%A1%B0%EA%B1%B4%EB%B6%80_%ED%99%95%EB%A5%A0.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A = 학생이 남학생인 사건\n",
    "- B = 학생이 여학생인 사건\n",
    "- C = 학생이 중학생인 사건\n",
    "- D = 학생이 고등학생인 사건\n",
    "이라고 했을 때, 아래의 확률을 계산해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 학생을 뽑았을 때, 남학생일 확률<br/>\n",
    ": P(A)=180/360=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 학생을 뽑았을 때, 고등학생이면서 남학생일 확률<br/>\n",
    ": P(A∩D) = 80/360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 고등학생 중 한명을 뽑았을 때, 남학생일 확률<br/>\n",
    ": P(A|D) = 80/200 = P(A∩D)/P(D) = (80/360)/(200/360) = 80/200 = 0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
